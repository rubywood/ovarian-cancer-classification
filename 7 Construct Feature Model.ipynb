{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3901ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.layers.helpers import to_2tuple\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import openslide\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e10f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = 2\n",
    "\n",
    "DATA_DIR = 'Data/'\n",
    "\n",
    "args = pd.Series({\n",
    "    'batch_size_per_gpu' : 16, #512,\n",
    "    'num_workers': 8,\n",
    "    'image_dir': os.path.join(DATA_DIR, 'train_images'),\n",
    "    'train_val_split': 0.8,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f16c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f'cuda:{gpu}' if torch.cuda.is_available() else 'cpu')    \n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbbe773",
   "metadata": {},
   "source": [
    "CTransPath from https://github.com/Xiyue-Wang/TransPath\n",
    "\n",
    "Make sure to import specified version of timm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee91d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvStem(nn.Module):\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=768, norm_layer=None, \n",
    "                 flatten=True, output_fmt=None):\n",
    "        super().__init__()\n",
    "\n",
    "        assert patch_size == 4\n",
    "        assert embed_dim % 8 == 0\n",
    "\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.flatten = flatten\n",
    "\n",
    "\n",
    "        stem = []\n",
    "        input_dim, output_dim = 3, embed_dim // 8\n",
    "        for l in range(2):\n",
    "            stem.append(nn.Conv2d(input_dim, output_dim, kernel_size=3, stride=2, padding=1, bias=False))\n",
    "            stem.append(nn.BatchNorm2d(output_dim))\n",
    "            stem.append(nn.ReLU(inplace=True))\n",
    "            input_dim = output_dim\n",
    "            output_dim *= 2\n",
    "        stem.append(nn.Conv2d(input_dim, embed_dim, kernel_size=1))\n",
    "        self.proj = nn.Sequential(*stem)\n",
    "\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x)\n",
    "        if self.flatten:\n",
    "            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "def ctranspath():\n",
    "    model = timm.create_model('swin_tiny_patch4_window7_224', embed_layer=ConvStem, pretrained=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e3b3e0",
   "metadata": {},
   "source": [
    "Look at model breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71e2cd7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 224, 224])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 12, 112, 112]             324\n",
      "       BatchNorm2d-2         [-1, 12, 112, 112]              24\n",
      "              ReLU-3         [-1, 12, 112, 112]               0\n",
      "            Conv2d-4           [-1, 24, 56, 56]           2,592\n",
      "       BatchNorm2d-5           [-1, 24, 56, 56]              48\n",
      "              ReLU-6           [-1, 24, 56, 56]               0\n",
      "            Conv2d-7           [-1, 96, 56, 56]           2,400\n",
      "         LayerNorm-8             [-1, 3136, 96]             192\n",
      "          ConvStem-9             [-1, 3136, 96]               0\n",
      "          Dropout-10             [-1, 3136, 96]               0\n",
      "        LayerNorm-11             [-1, 3136, 96]             192\n",
      "           Linear-12              [-1, 49, 288]          27,936\n",
      "          Softmax-13            [-1, 3, 49, 49]               0\n",
      "          Dropout-14            [-1, 3, 49, 49]               0\n",
      "           Linear-15               [-1, 49, 96]           9,312\n",
      "          Dropout-16               [-1, 49, 96]               0\n",
      "  WindowAttention-17               [-1, 49, 96]               0\n",
      "         Identity-18             [-1, 3136, 96]               0\n",
      "        LayerNorm-19             [-1, 3136, 96]             192\n",
      "           Linear-20            [-1, 3136, 384]          37,248\n",
      "             GELU-21            [-1, 3136, 384]               0\n",
      "          Dropout-22            [-1, 3136, 384]               0\n",
      "           Linear-23             [-1, 3136, 96]          36,960\n",
      "          Dropout-24             [-1, 3136, 96]               0\n",
      "              Mlp-25             [-1, 3136, 96]               0\n",
      "         Identity-26             [-1, 3136, 96]               0\n",
      "SwinTransformerBlock-27             [-1, 3136, 96]               0\n",
      "        LayerNorm-28             [-1, 3136, 96]             192\n",
      "           Linear-29              [-1, 49, 288]          27,936\n",
      "          Softmax-30            [-1, 3, 49, 49]               0\n",
      "          Dropout-31            [-1, 3, 49, 49]               0\n",
      "           Linear-32               [-1, 49, 96]           9,312\n",
      "          Dropout-33               [-1, 49, 96]               0\n",
      "  WindowAttention-34               [-1, 49, 96]               0\n",
      "         DropPath-35             [-1, 3136, 96]               0\n",
      "        LayerNorm-36             [-1, 3136, 96]             192\n",
      "           Linear-37            [-1, 3136, 384]          37,248\n",
      "             GELU-38            [-1, 3136, 384]               0\n",
      "          Dropout-39            [-1, 3136, 384]               0\n",
      "           Linear-40             [-1, 3136, 96]          36,960\n",
      "          Dropout-41             [-1, 3136, 96]               0\n",
      "              Mlp-42             [-1, 3136, 96]               0\n",
      "         DropPath-43             [-1, 3136, 96]               0\n",
      "SwinTransformerBlock-44             [-1, 3136, 96]               0\n",
      "        LayerNorm-45             [-1, 784, 384]             768\n",
      "           Linear-46             [-1, 784, 192]          73,728\n",
      "     PatchMerging-47             [-1, 784, 192]               0\n",
      "       BasicLayer-48             [-1, 784, 192]               0\n",
      "        LayerNorm-49             [-1, 784, 192]             384\n",
      "           Linear-50              [-1, 49, 576]         111,168\n",
      "          Softmax-51            [-1, 6, 49, 49]               0\n",
      "          Dropout-52            [-1, 6, 49, 49]               0\n",
      "           Linear-53              [-1, 49, 192]          37,056\n",
      "          Dropout-54              [-1, 49, 192]               0\n",
      "  WindowAttention-55              [-1, 49, 192]               0\n",
      "         DropPath-56             [-1, 784, 192]               0\n",
      "        LayerNorm-57             [-1, 784, 192]             384\n",
      "           Linear-58             [-1, 784, 768]         148,224\n",
      "             GELU-59             [-1, 784, 768]               0\n",
      "          Dropout-60             [-1, 784, 768]               0\n",
      "           Linear-61             [-1, 784, 192]         147,648\n",
      "          Dropout-62             [-1, 784, 192]               0\n",
      "              Mlp-63             [-1, 784, 192]               0\n",
      "         DropPath-64             [-1, 784, 192]               0\n",
      "SwinTransformerBlock-65             [-1, 784, 192]               0\n",
      "        LayerNorm-66             [-1, 784, 192]             384\n",
      "           Linear-67              [-1, 49, 576]         111,168\n",
      "          Softmax-68            [-1, 6, 49, 49]               0\n",
      "          Dropout-69            [-1, 6, 49, 49]               0\n",
      "           Linear-70              [-1, 49, 192]          37,056\n",
      "          Dropout-71              [-1, 49, 192]               0\n",
      "  WindowAttention-72              [-1, 49, 192]               0\n",
      "         DropPath-73             [-1, 784, 192]               0\n",
      "        LayerNorm-74             [-1, 784, 192]             384\n",
      "           Linear-75             [-1, 784, 768]         148,224\n",
      "             GELU-76             [-1, 784, 768]               0\n",
      "          Dropout-77             [-1, 784, 768]               0\n",
      "           Linear-78             [-1, 784, 192]         147,648\n",
      "          Dropout-79             [-1, 784, 192]               0\n",
      "              Mlp-80             [-1, 784, 192]               0\n",
      "         DropPath-81             [-1, 784, 192]               0\n",
      "SwinTransformerBlock-82             [-1, 784, 192]               0\n",
      "        LayerNorm-83             [-1, 196, 768]           1,536\n",
      "           Linear-84             [-1, 196, 384]         294,912\n",
      "     PatchMerging-85             [-1, 196, 384]               0\n",
      "       BasicLayer-86             [-1, 196, 384]               0\n",
      "        LayerNorm-87             [-1, 196, 384]             768\n",
      "           Linear-88             [-1, 49, 1152]         443,520\n",
      "          Softmax-89           [-1, 12, 49, 49]               0\n",
      "          Dropout-90           [-1, 12, 49, 49]               0\n",
      "           Linear-91              [-1, 49, 384]         147,840\n",
      "          Dropout-92              [-1, 49, 384]               0\n",
      "  WindowAttention-93              [-1, 49, 384]               0\n",
      "         DropPath-94             [-1, 196, 384]               0\n",
      "        LayerNorm-95             [-1, 196, 384]             768\n",
      "           Linear-96            [-1, 196, 1536]         591,360\n",
      "             GELU-97            [-1, 196, 1536]               0\n",
      "          Dropout-98            [-1, 196, 1536]               0\n",
      "           Linear-99             [-1, 196, 384]         590,208\n",
      "         Dropout-100             [-1, 196, 384]               0\n",
      "             Mlp-101             [-1, 196, 384]               0\n",
      "        DropPath-102             [-1, 196, 384]               0\n",
      "SwinTransformerBlock-103             [-1, 196, 384]               0\n",
      "       LayerNorm-104             [-1, 196, 384]             768\n",
      "          Linear-105             [-1, 49, 1152]         443,520\n",
      "         Softmax-106           [-1, 12, 49, 49]               0\n",
      "         Dropout-107           [-1, 12, 49, 49]               0\n",
      "          Linear-108              [-1, 49, 384]         147,840\n",
      "         Dropout-109              [-1, 49, 384]               0\n",
      " WindowAttention-110              [-1, 49, 384]               0\n",
      "        DropPath-111             [-1, 196, 384]               0\n",
      "       LayerNorm-112             [-1, 196, 384]             768\n",
      "          Linear-113            [-1, 196, 1536]         591,360\n",
      "            GELU-114            [-1, 196, 1536]               0\n",
      "         Dropout-115            [-1, 196, 1536]               0\n",
      "          Linear-116             [-1, 196, 384]         590,208\n",
      "         Dropout-117             [-1, 196, 384]               0\n",
      "             Mlp-118             [-1, 196, 384]               0\n",
      "        DropPath-119             [-1, 196, 384]               0\n",
      "SwinTransformerBlock-120             [-1, 196, 384]               0\n",
      "       LayerNorm-121             [-1, 196, 384]             768\n",
      "          Linear-122             [-1, 49, 1152]         443,520\n",
      "         Softmax-123           [-1, 12, 49, 49]               0\n",
      "         Dropout-124           [-1, 12, 49, 49]               0\n",
      "          Linear-125              [-1, 49, 384]         147,840\n",
      "         Dropout-126              [-1, 49, 384]               0\n",
      " WindowAttention-127              [-1, 49, 384]               0\n",
      "        DropPath-128             [-1, 196, 384]               0\n",
      "       LayerNorm-129             [-1, 196, 384]             768\n",
      "          Linear-130            [-1, 196, 1536]         591,360\n",
      "            GELU-131            [-1, 196, 1536]               0\n",
      "         Dropout-132            [-1, 196, 1536]               0\n",
      "          Linear-133             [-1, 196, 384]         590,208\n",
      "         Dropout-134             [-1, 196, 384]               0\n",
      "             Mlp-135             [-1, 196, 384]               0\n",
      "        DropPath-136             [-1, 196, 384]               0\n",
      "SwinTransformerBlock-137             [-1, 196, 384]               0\n",
      "       LayerNorm-138             [-1, 196, 384]             768\n",
      "          Linear-139             [-1, 49, 1152]         443,520\n",
      "         Softmax-140           [-1, 12, 49, 49]               0\n",
      "         Dropout-141           [-1, 12, 49, 49]               0\n",
      "          Linear-142              [-1, 49, 384]         147,840\n",
      "         Dropout-143              [-1, 49, 384]               0\n",
      " WindowAttention-144              [-1, 49, 384]               0\n",
      "        DropPath-145             [-1, 196, 384]               0\n",
      "       LayerNorm-146             [-1, 196, 384]             768\n",
      "          Linear-147            [-1, 196, 1536]         591,360\n",
      "            GELU-148            [-1, 196, 1536]               0\n",
      "         Dropout-149            [-1, 196, 1536]               0\n",
      "          Linear-150             [-1, 196, 384]         590,208\n",
      "         Dropout-151             [-1, 196, 384]               0\n",
      "             Mlp-152             [-1, 196, 384]               0\n",
      "        DropPath-153             [-1, 196, 384]               0\n",
      "SwinTransformerBlock-154             [-1, 196, 384]               0\n",
      "       LayerNorm-155             [-1, 196, 384]             768\n",
      "          Linear-156             [-1, 49, 1152]         443,520\n",
      "         Softmax-157           [-1, 12, 49, 49]               0\n",
      "         Dropout-158           [-1, 12, 49, 49]               0\n",
      "          Linear-159              [-1, 49, 384]         147,840\n",
      "         Dropout-160              [-1, 49, 384]               0\n",
      " WindowAttention-161              [-1, 49, 384]               0\n",
      "        DropPath-162             [-1, 196, 384]               0\n",
      "       LayerNorm-163             [-1, 196, 384]             768\n",
      "          Linear-164            [-1, 196, 1536]         591,360\n",
      "            GELU-165            [-1, 196, 1536]               0\n",
      "         Dropout-166            [-1, 196, 1536]               0\n",
      "          Linear-167             [-1, 196, 384]         590,208\n",
      "         Dropout-168             [-1, 196, 384]               0\n",
      "             Mlp-169             [-1, 196, 384]               0\n",
      "        DropPath-170             [-1, 196, 384]               0\n",
      "SwinTransformerBlock-171             [-1, 196, 384]               0\n",
      "       LayerNorm-172             [-1, 196, 384]             768\n",
      "          Linear-173             [-1, 49, 1152]         443,520\n",
      "         Softmax-174           [-1, 12, 49, 49]               0\n",
      "         Dropout-175           [-1, 12, 49, 49]               0\n",
      "          Linear-176              [-1, 49, 384]         147,840\n",
      "         Dropout-177              [-1, 49, 384]               0\n",
      " WindowAttention-178              [-1, 49, 384]               0\n",
      "        DropPath-179             [-1, 196, 384]               0\n",
      "       LayerNorm-180             [-1, 196, 384]             768\n",
      "          Linear-181            [-1, 196, 1536]         591,360\n",
      "            GELU-182            [-1, 196, 1536]               0\n",
      "         Dropout-183            [-1, 196, 1536]               0\n",
      "          Linear-184             [-1, 196, 384]         590,208\n",
      "         Dropout-185             [-1, 196, 384]               0\n",
      "             Mlp-186             [-1, 196, 384]               0\n",
      "        DropPath-187             [-1, 196, 384]               0\n",
      "SwinTransformerBlock-188             [-1, 196, 384]               0\n",
      "       LayerNorm-189             [-1, 49, 1536]           3,072\n",
      "          Linear-190              [-1, 49, 768]       1,179,648\n",
      "    PatchMerging-191              [-1, 49, 768]               0\n",
      "      BasicLayer-192              [-1, 49, 768]               0\n",
      "       LayerNorm-193              [-1, 49, 768]           1,536\n",
      "          Linear-194             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-195           [-1, 24, 49, 49]               0\n",
      "         Dropout-196           [-1, 24, 49, 49]               0\n",
      "          Linear-197              [-1, 49, 768]         590,592\n",
      "         Dropout-198              [-1, 49, 768]               0\n",
      " WindowAttention-199              [-1, 49, 768]               0\n",
      "        DropPath-200              [-1, 49, 768]               0\n",
      "       LayerNorm-201              [-1, 49, 768]           1,536\n",
      "          Linear-202             [-1, 49, 3072]       2,362,368\n",
      "            GELU-203             [-1, 49, 3072]               0\n",
      "         Dropout-204             [-1, 49, 3072]               0\n",
      "          Linear-205              [-1, 49, 768]       2,360,064\n",
      "         Dropout-206              [-1, 49, 768]               0\n",
      "             Mlp-207              [-1, 49, 768]               0\n",
      "        DropPath-208              [-1, 49, 768]               0\n",
      "SwinTransformerBlock-209              [-1, 49, 768]               0\n",
      "       LayerNorm-210              [-1, 49, 768]           1,536\n",
      "          Linear-211             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-212           [-1, 24, 49, 49]               0\n",
      "         Dropout-213           [-1, 24, 49, 49]               0\n",
      "          Linear-214              [-1, 49, 768]         590,592\n",
      "         Dropout-215              [-1, 49, 768]               0\n",
      " WindowAttention-216              [-1, 49, 768]               0\n",
      "        DropPath-217              [-1, 49, 768]               0\n",
      "       LayerNorm-218              [-1, 49, 768]           1,536\n",
      "          Linear-219             [-1, 49, 3072]       2,362,368\n",
      "            GELU-220             [-1, 49, 3072]               0\n",
      "         Dropout-221             [-1, 49, 3072]               0\n",
      "          Linear-222              [-1, 49, 768]       2,360,064\n",
      "         Dropout-223              [-1, 49, 768]               0\n",
      "             Mlp-224              [-1, 49, 768]               0\n",
      "        DropPath-225              [-1, 49, 768]               0\n",
      "SwinTransformerBlock-226              [-1, 49, 768]               0\n",
      "      BasicLayer-227              [-1, 49, 768]               0\n",
      "       LayerNorm-228              [-1, 49, 768]           1,536\n",
      "AdaptiveAvgPool1d-229               [-1, 768, 1]               0\n",
      "          Linear-230                 [-1, 1000]         769,000\n",
      "================================================================\n",
      "Total params: 28,265,716\n",
      "Trainable params: 28,265,716\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 258.16\n",
      "Params size (MB): 107.83\n",
      "Estimated Total Size (MB): 366.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = ctranspath()\n",
    "summary(model.cuda(), ( 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ab4fe72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  (patch_embed): ConvStem(\n",
       "    (proj): Sequential(\n",
       "      (0): Conv2d(3, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(12, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): SwinTransformerStage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.009)\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.009)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): SwinTransformerStage(\n",
       "      (downsample): PatchMerging(\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.018)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.018)\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.027)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.027)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): SwinTransformerStage(\n",
       "      (downsample): PatchMerging(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.036)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.036)\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.045)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.045)\n",
       "        )\n",
       "        (2): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.055)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.055)\n",
       "        )\n",
       "        (3): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.064)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.064)\n",
       "        )\n",
       "        (4): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.073)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.073)\n",
       "        )\n",
       "        (5): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.082)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.082)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): SwinTransformerStage(\n",
       "      (downsample): PatchMerging(\n",
       "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.091)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.091)\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.100)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate=none)\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): ClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
       "    (flatten): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abba583d",
   "metadata": {},
   "source": [
    "Load pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88935eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTransPath model built.\n"
     ]
    }
   ],
   "source": [
    "# ============ building network ... ============\n",
    "model = ctranspath()\n",
    "model.head = nn.Identity()\n",
    "# load weights to evaluate\n",
    "td = torch.load(r'ctranspath.pth')\n",
    "model.load_state_dict(td['model'], strict=True)\n",
    "\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "#embed_dim = model.embed_dim # returns 96 for initial conv layer but actually might be 768\n",
    "embed_dim = model.layers[-1].blocks[-1].mlp.fc2.out_features # this is 768\n",
    "\n",
    "print(f\"CTransPath model built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9f55a0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 224, 224])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 12, 112, 112]             324\n",
      "       BatchNorm2d-2         [-1, 12, 112, 112]              24\n",
      "              ReLU-3         [-1, 12, 112, 112]               0\n",
      "            Conv2d-4           [-1, 24, 56, 56]           2,592\n",
      "       BatchNorm2d-5           [-1, 24, 56, 56]              48\n",
      "              ReLU-6           [-1, 24, 56, 56]               0\n",
      "            Conv2d-7           [-1, 96, 56, 56]           2,400\n",
      "         LayerNorm-8             [-1, 3136, 96]             192\n",
      "          ConvStem-9             [-1, 3136, 96]               0\n",
      "          Dropout-10             [-1, 3136, 96]               0\n",
      "        LayerNorm-11             [-1, 3136, 96]             192\n",
      "           Linear-12              [-1, 49, 288]          27,936\n",
      "          Softmax-13            [-1, 3, 49, 49]               0\n",
      "          Dropout-14            [-1, 3, 49, 49]               0\n",
      "           Linear-15               [-1, 49, 96]           9,312\n",
      "          Dropout-16               [-1, 49, 96]               0\n",
      "  WindowAttention-17               [-1, 49, 96]               0\n",
      "         Identity-18             [-1, 3136, 96]               0\n",
      "        LayerNorm-19             [-1, 3136, 96]             192\n",
      "           Linear-20            [-1, 3136, 384]          37,248\n",
      "             GELU-21            [-1, 3136, 384]               0\n",
      "          Dropout-22            [-1, 3136, 384]               0\n",
      "           Linear-23             [-1, 3136, 96]          36,960\n",
      "          Dropout-24             [-1, 3136, 96]               0\n",
      "              Mlp-25             [-1, 3136, 96]               0\n",
      "         Identity-26             [-1, 3136, 96]               0\n",
      "SwinTransformerBlock-27             [-1, 3136, 96]               0\n",
      "        LayerNorm-28             [-1, 3136, 96]             192\n",
      "           Linear-29              [-1, 49, 288]          27,936\n",
      "          Softmax-30            [-1, 3, 49, 49]               0\n",
      "          Dropout-31            [-1, 3, 49, 49]               0\n",
      "           Linear-32               [-1, 49, 96]           9,312\n",
      "          Dropout-33               [-1, 49, 96]               0\n",
      "  WindowAttention-34               [-1, 49, 96]               0\n",
      "         DropPath-35             [-1, 3136, 96]               0\n",
      "        LayerNorm-36             [-1, 3136, 96]             192\n",
      "           Linear-37            [-1, 3136, 384]          37,248\n",
      "             GELU-38            [-1, 3136, 384]               0\n",
      "          Dropout-39            [-1, 3136, 384]               0\n",
      "           Linear-40             [-1, 3136, 96]          36,960\n",
      "          Dropout-41             [-1, 3136, 96]               0\n",
      "              Mlp-42             [-1, 3136, 96]               0\n",
      "         DropPath-43             [-1, 3136, 96]               0\n",
      "SwinTransformerBlock-44             [-1, 3136, 96]               0\n",
      "        LayerNorm-45             [-1, 784, 384]             768\n",
      "           Linear-46             [-1, 784, 192]          73,728\n",
      "     PatchMerging-47             [-1, 784, 192]               0\n",
      "       BasicLayer-48             [-1, 784, 192]               0\n",
      "        LayerNorm-49             [-1, 784, 192]             384\n",
      "           Linear-50              [-1, 49, 576]         111,168\n",
      "          Softmax-51            [-1, 6, 49, 49]               0\n",
      "          Dropout-52            [-1, 6, 49, 49]               0\n",
      "           Linear-53              [-1, 49, 192]          37,056\n",
      "          Dropout-54              [-1, 49, 192]               0\n",
      "  WindowAttention-55              [-1, 49, 192]               0\n",
      "         DropPath-56             [-1, 784, 192]               0\n",
      "        LayerNorm-57             [-1, 784, 192]             384\n",
      "           Linear-58             [-1, 784, 768]         148,224\n",
      "             GELU-59             [-1, 784, 768]               0\n",
      "          Dropout-60             [-1, 784, 768]               0\n",
      "           Linear-61             [-1, 784, 192]         147,648\n",
      "          Dropout-62             [-1, 784, 192]               0\n",
      "              Mlp-63             [-1, 784, 192]               0\n",
      "         DropPath-64             [-1, 784, 192]               0\n",
      "SwinTransformerBlock-65             [-1, 784, 192]               0\n",
      "        LayerNorm-66             [-1, 784, 192]             384\n",
      "           Linear-67              [-1, 49, 576]         111,168\n",
      "          Softmax-68            [-1, 6, 49, 49]               0\n",
      "          Dropout-69            [-1, 6, 49, 49]               0\n",
      "           Linear-70              [-1, 49, 192]          37,056\n",
      "          Dropout-71              [-1, 49, 192]               0\n",
      "  WindowAttention-72              [-1, 49, 192]               0\n",
      "         DropPath-73             [-1, 784, 192]               0\n",
      "        LayerNorm-74             [-1, 784, 192]             384\n",
      "           Linear-75             [-1, 784, 768]         148,224\n",
      "             GELU-76             [-1, 784, 768]               0\n",
      "          Dropout-77             [-1, 784, 768]               0\n",
      "           Linear-78             [-1, 784, 192]         147,648\n",
      "          Dropout-79             [-1, 784, 192]               0\n",
      "              Mlp-80             [-1, 784, 192]               0\n",
      "         DropPath-81             [-1, 784, 192]               0\n",
      "SwinTransformerBlock-82             [-1, 784, 192]               0\n",
      "        LayerNorm-83             [-1, 196, 768]           1,536\n",
      "           Linear-84             [-1, 196, 384]         294,912\n",
      "     PatchMerging-85             [-1, 196, 384]               0\n",
      "       BasicLayer-86             [-1, 196, 384]               0\n",
      "        LayerNorm-87             [-1, 196, 384]             768\n",
      "           Linear-88             [-1, 49, 1152]         443,520\n",
      "          Softmax-89           [-1, 12, 49, 49]               0\n",
      "          Dropout-90           [-1, 12, 49, 49]               0\n",
      "           Linear-91              [-1, 49, 384]         147,840\n",
      "          Dropout-92              [-1, 49, 384]               0\n",
      "  WindowAttention-93              [-1, 49, 384]               0\n",
      "         DropPath-94             [-1, 196, 384]               0\n",
      "        LayerNorm-95             [-1, 196, 384]             768\n",
      "           Linear-96            [-1, 196, 1536]         591,360\n",
      "             GELU-97            [-1, 196, 1536]               0\n",
      "          Dropout-98            [-1, 196, 1536]               0\n",
      "           Linear-99             [-1, 196, 384]         590,208\n",
      "         Dropout-100             [-1, 196, 384]               0\n",
      "             Mlp-101             [-1, 196, 384]               0\n",
      "        DropPath-102             [-1, 196, 384]               0\n",
      "SwinTransformerBlock-103             [-1, 196, 384]               0\n",
      "       LayerNorm-104             [-1, 196, 384]             768\n",
      "          Linear-105             [-1, 49, 1152]         443,520\n",
      "         Softmax-106           [-1, 12, 49, 49]               0\n",
      "         Dropout-107           [-1, 12, 49, 49]               0\n",
      "          Linear-108              [-1, 49, 384]         147,840\n",
      "         Dropout-109              [-1, 49, 384]               0\n",
      " WindowAttention-110              [-1, 49, 384]               0\n",
      "        DropPath-111             [-1, 196, 384]               0\n",
      "       LayerNorm-112             [-1, 196, 384]             768\n",
      "          Linear-113            [-1, 196, 1536]         591,360\n",
      "            GELU-114            [-1, 196, 1536]               0\n",
      "         Dropout-115            [-1, 196, 1536]               0\n",
      "          Linear-116             [-1, 196, 384]         590,208\n",
      "         Dropout-117             [-1, 196, 384]               0\n",
      "             Mlp-118             [-1, 196, 384]               0\n",
      "        DropPath-119             [-1, 196, 384]               0\n",
      "SwinTransformerBlock-120             [-1, 196, 384]               0\n",
      "       LayerNorm-121             [-1, 196, 384]             768\n",
      "          Linear-122             [-1, 49, 1152]         443,520\n",
      "         Softmax-123           [-1, 12, 49, 49]               0\n",
      "         Dropout-124           [-1, 12, 49, 49]               0\n",
      "          Linear-125              [-1, 49, 384]         147,840\n",
      "         Dropout-126              [-1, 49, 384]               0\n",
      " WindowAttention-127              [-1, 49, 384]               0\n",
      "        DropPath-128             [-1, 196, 384]               0\n",
      "       LayerNorm-129             [-1, 196, 384]             768\n",
      "          Linear-130            [-1, 196, 1536]         591,360\n",
      "            GELU-131            [-1, 196, 1536]               0\n",
      "         Dropout-132            [-1, 196, 1536]               0\n",
      "          Linear-133             [-1, 196, 384]         590,208\n",
      "         Dropout-134             [-1, 196, 384]               0\n",
      "             Mlp-135             [-1, 196, 384]               0\n",
      "        DropPath-136             [-1, 196, 384]               0\n",
      "SwinTransformerBlock-137             [-1, 196, 384]               0\n",
      "       LayerNorm-138             [-1, 196, 384]             768\n",
      "          Linear-139             [-1, 49, 1152]         443,520\n",
      "         Softmax-140           [-1, 12, 49, 49]               0\n",
      "         Dropout-141           [-1, 12, 49, 49]               0\n",
      "          Linear-142              [-1, 49, 384]         147,840\n",
      "         Dropout-143              [-1, 49, 384]               0\n",
      " WindowAttention-144              [-1, 49, 384]               0\n",
      "        DropPath-145             [-1, 196, 384]               0\n",
      "       LayerNorm-146             [-1, 196, 384]             768\n",
      "          Linear-147            [-1, 196, 1536]         591,360\n",
      "            GELU-148            [-1, 196, 1536]               0\n",
      "         Dropout-149            [-1, 196, 1536]               0\n",
      "          Linear-150             [-1, 196, 384]         590,208\n",
      "         Dropout-151             [-1, 196, 384]               0\n",
      "             Mlp-152             [-1, 196, 384]               0\n",
      "        DropPath-153             [-1, 196, 384]               0\n",
      "SwinTransformerBlock-154             [-1, 196, 384]               0\n",
      "       LayerNorm-155             [-1, 196, 384]             768\n",
      "          Linear-156             [-1, 49, 1152]         443,520\n",
      "         Softmax-157           [-1, 12, 49, 49]               0\n",
      "         Dropout-158           [-1, 12, 49, 49]               0\n",
      "          Linear-159              [-1, 49, 384]         147,840\n",
      "         Dropout-160              [-1, 49, 384]               0\n",
      " WindowAttention-161              [-1, 49, 384]               0\n",
      "        DropPath-162             [-1, 196, 384]               0\n",
      "       LayerNorm-163             [-1, 196, 384]             768\n",
      "          Linear-164            [-1, 196, 1536]         591,360\n",
      "            GELU-165            [-1, 196, 1536]               0\n",
      "         Dropout-166            [-1, 196, 1536]               0\n",
      "          Linear-167             [-1, 196, 384]         590,208\n",
      "         Dropout-168             [-1, 196, 384]               0\n",
      "             Mlp-169             [-1, 196, 384]               0\n",
      "        DropPath-170             [-1, 196, 384]               0\n",
      "SwinTransformerBlock-171             [-1, 196, 384]               0\n",
      "       LayerNorm-172             [-1, 196, 384]             768\n",
      "          Linear-173             [-1, 49, 1152]         443,520\n",
      "         Softmax-174           [-1, 12, 49, 49]               0\n",
      "         Dropout-175           [-1, 12, 49, 49]               0\n",
      "          Linear-176              [-1, 49, 384]         147,840\n",
      "         Dropout-177              [-1, 49, 384]               0\n",
      " WindowAttention-178              [-1, 49, 384]               0\n",
      "        DropPath-179             [-1, 196, 384]               0\n",
      "       LayerNorm-180             [-1, 196, 384]             768\n",
      "          Linear-181            [-1, 196, 1536]         591,360\n",
      "            GELU-182            [-1, 196, 1536]               0\n",
      "         Dropout-183            [-1, 196, 1536]               0\n",
      "          Linear-184             [-1, 196, 384]         590,208\n",
      "         Dropout-185             [-1, 196, 384]               0\n",
      "             Mlp-186             [-1, 196, 384]               0\n",
      "        DropPath-187             [-1, 196, 384]               0\n",
      "SwinTransformerBlock-188             [-1, 196, 384]               0\n",
      "       LayerNorm-189             [-1, 49, 1536]           3,072\n",
      "          Linear-190              [-1, 49, 768]       1,179,648\n",
      "    PatchMerging-191              [-1, 49, 768]               0\n",
      "      BasicLayer-192              [-1, 49, 768]               0\n",
      "       LayerNorm-193              [-1, 49, 768]           1,536\n",
      "          Linear-194             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-195           [-1, 24, 49, 49]               0\n",
      "         Dropout-196           [-1, 24, 49, 49]               0\n",
      "          Linear-197              [-1, 49, 768]         590,592\n",
      "         Dropout-198              [-1, 49, 768]               0\n",
      " WindowAttention-199              [-1, 49, 768]               0\n",
      "        DropPath-200              [-1, 49, 768]               0\n",
      "       LayerNorm-201              [-1, 49, 768]           1,536\n",
      "          Linear-202             [-1, 49, 3072]       2,362,368\n",
      "            GELU-203             [-1, 49, 3072]               0\n",
      "         Dropout-204             [-1, 49, 3072]               0\n",
      "          Linear-205              [-1, 49, 768]       2,360,064\n",
      "         Dropout-206              [-1, 49, 768]               0\n",
      "             Mlp-207              [-1, 49, 768]               0\n",
      "        DropPath-208              [-1, 49, 768]               0\n",
      "SwinTransformerBlock-209              [-1, 49, 768]               0\n",
      "       LayerNorm-210              [-1, 49, 768]           1,536\n",
      "          Linear-211             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-212           [-1, 24, 49, 49]               0\n",
      "         Dropout-213           [-1, 24, 49, 49]               0\n",
      "          Linear-214              [-1, 49, 768]         590,592\n",
      "         Dropout-215              [-1, 49, 768]               0\n",
      " WindowAttention-216              [-1, 49, 768]               0\n",
      "        DropPath-217              [-1, 49, 768]               0\n",
      "       LayerNorm-218              [-1, 49, 768]           1,536\n",
      "          Linear-219             [-1, 49, 3072]       2,362,368\n",
      "            GELU-220             [-1, 49, 3072]               0\n",
      "         Dropout-221             [-1, 49, 3072]               0\n",
      "          Linear-222              [-1, 49, 768]       2,360,064\n",
      "         Dropout-223              [-1, 49, 768]               0\n",
      "             Mlp-224              [-1, 49, 768]               0\n",
      "        DropPath-225              [-1, 49, 768]               0\n",
      "SwinTransformerBlock-226              [-1, 49, 768]               0\n",
      "      BasicLayer-227              [-1, 49, 768]               0\n",
      "       LayerNorm-228              [-1, 49, 768]           1,536\n",
      "AdaptiveAvgPool1d-229               [-1, 768, 1]               0\n",
      "        Identity-230                  [-1, 768]               0\n",
      "================================================================\n",
      "Total params: 27,496,716\n",
      "Trainable params: 27,496,716\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 258.15\n",
      "Params size (MB): 104.89\n",
      "Estimated Total Size (MB): 363.62\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, ( 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7790500d",
   "metadata": {},
   "source": [
    "Add model head in training for the output we want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b874dc1",
   "metadata": {},
   "source": [
    "### Check passing data into model\n",
    "\n",
    "#### Construct data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de4c01",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d51ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_df = pd.read_csv('train_patches.csv', index_col=0)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, 'train_hot.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef20f02e",
   "metadata": {},
   "source": [
    "Define transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9705baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRotateTransform:\n",
    "    def __init__(self, angles):  #: Sequence[int]):\n",
    "        self.angles = angles\n",
    "\n",
    "    def __call__(self, x):\n",
    "        angle = random.choice(self.angles)\n",
    "        return transforms.functional.rotate(x, angle)\n",
    "    \n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(.1, 0.25, 0.5, 0.25),\n",
    "    transforms.GaussianBlur(kernel_size=(9, 9)),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2., p=0.2),\n",
    "    transforms.RandomAutocontrast(p=0.5),\n",
    "    MyRotateTransform(angles=[-90, 0, 90, 180]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49713c7a",
   "metadata": {},
   "source": [
    "Define dataset and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f4b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, patch_df, label_df, image_folder, transform=None, patch_size=256):\n",
    "        self.patch_df = patch_df\n",
    "        self.label_df = label_df\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.patch_df)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        patch_info = self.patch_df.loc[idx] # using loc, so using name of index not ordered position\n",
    "        x, y = patch_info['x_coord'], patch_info['y_coord']\n",
    "        image_id = patch_info['image_id']\n",
    "        \n",
    "        wsi = openslide.OpenSlide(os.path.join(self.image_folder, f\"{image_id}.tif\"))\n",
    "        wsi_size = wsi.dimensions\n",
    "        \n",
    "        patch = wsi.read_region((x, y), level=0, size=(self.patch_size, self.patch_size))\n",
    "        patch = np.array(patch)[..., :3]\n",
    "        \n",
    "        \n",
    "        # shape torch.Size([batch_size, 256, 256, 3])\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            pil_img = Image.fromarray(patch)\n",
    "            patch = self.transform(pil_img) # pil_img\n",
    "        \n",
    "        label = self.label_for_id(image_id)\n",
    "        \n",
    "        return patch, label\n",
    "    \n",
    "        \n",
    "    def label_for_id(self, image_id):\n",
    "        return self.label_df[self.label_df['image_id']==image_id]['label_cat'].iloc[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "014ede8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_subset = PatchDataset(patch_df=patch_df.iloc[:1000], label_df=train_df, \n",
    "                             image_folder=args.image_dir, transform=val_transform)\n",
    "        \n",
    "patch_loader = torch.utils.data.DataLoader(patch_subset, batch_size=args.batch_size_per_gpu, \n",
    "                                           shuffle=False, num_workers=args.num_workers, \n",
    "                                           pin_memory=True, sampler=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c7164",
   "metadata": {},
   "source": [
    "#### Pass input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ddbd7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "slide_features = []\n",
    "labels = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for i, (inp, lbl) in enumerate(patch_loader):\n",
    "    inp = inp.to(device)\n",
    "    \n",
    "    labels += lbl\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(inp)\n",
    "        slide_features += output\n",
    "\n",
    "slide_features = torch.vstack(slide_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "07f9bcd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 768])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slide_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00e0589c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 768])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slide_features.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
